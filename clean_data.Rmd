---
title: "clean_data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(shinythemes)
library(tidycensus)
library(rstanarm)
census_api_key("fcc099ffb2714e2423437589e23236c5640ba53e")
```

```{r census_pull}

# There were parts of my shiny app that took a long time to run. In order to
# save time, Tyler recommended that I run these portions in another R markdown
# file and save their outputs. That way, it can be much faster when I run my
# shiny app. At first, this was just to help with pulling census data like I do
# in this r chunk. Later, I added on the posterior_predict outputs and the
# ozcensus_combined_all object, after trying to include the models and object
# in the renderPlot({}) in the server.

census_pull <- function(state) {
  get_acs(geography = "tract",
          state = state,
          variables = "B19013_001",
          year = 2018,
          geometry = TRUE)  
}

alcensus <- census_pull("AL")
nccensus <- census_pull("NC")
sccensus <- census_pull("SC")
tncensus <- census_pull("TN")
gacensus <- census_pull("GA")
mscensus <- census_pull("MS")
census1 <- bind_rows(alcensus, nccensus, sccensus,
                    tncensus, gacensus, mscensus)

  # Here, I created a function to pull data from the census by state. The census
  # data is from the 2018 American Community Survey. I chose a variable at
  # random, because all I need is the geometry, so that I can plot the census
  # tract's designation as an Opportunity Zone on the map.

# Save that output to a file, so that I don't have to run it every time the app
# runs.

saveRDS(census1, 
        file = "Gov50final_data/censuspulldata.rds")

  # Now that I have saved these data to a file, I can read that file in the
  # shiny app and use it easily.

```

```{r ozcensus_combined_all}
# Make object that combines ozlist and census data for all relevant states
  # This will be faster for the shiny app to do than running ozcensus_combine()
  # 6 times or some modified version of it.

      # For the sake of speed, I made a function to create the means and
      # standard deviations separately and combine them, so that the object made
      # by the function is ready to be graphed immediately in this output.

# Save this in the .Rds which will hold all objects from this .Rmd to be read
# into final_project.R

combine_all <- function(data = ozlist) {
  A <- data %>% 
    filter(state %in% states) %>% 
    select(state, GEOID, PovertyRate, HSorlower, medhhincome2014_tract, 
           pctwhitealone, unemprate, Designated) %>% 
    left_join(census, ., by = "GEOID") %>% 
    mutate(state = case_when(str_detect(NAME, "Alabama") ~ "Alabama",
                             str_detect(NAME, "North Carolina") ~ "North Carolina",
                             str_detect(NAME, "South Carolina") ~ "South Carolina",
                             str_detect(NAME, "Tennessee") ~ "Tennessee",
                             str_detect(NAME, "Georgia") ~ "Georgia",
                             str_detect(NAME, "Mississippi") ~ "Mississippi",
                             TRUE ~ "delete")) %>% 
    filter(state != "delete") %>% 
    filter(!is.na(Designated)) %>% 
    mutate_if(is.numeric, ~ replace(., is.na(.), 0)) %>% 
    group_by(Designated, state) %>% 
    summarize(unemprate = mean(unemprate,
                                na.rm = TRUE,
                                .groups = "keep"),
               PovertyRate = mean(PovertyRate,
                                  na.rm = TRUE,
                                  .groups = "keep"),
               medhhincome2014_tract = mean(medhhincome2014_tract,
                                            na.rm = TRUE,
                                            .groups = "keep"),
               HSorlower = mean(HSorlower,
                                na.rm = TRUE,
                                .groups = "keep"),
               pctwhitealone = mean(pctwhitealone,
                                    na.rm = TRUE,
                                    .groups = "keep"))
  
  B <- ozlist %>% 
    mutate(state = as.factor(state)) %>% 
    group_by(Designated, state) %>% 
    summarize(sd_ue = sd(unemprate,
                         na.rm = TRUE),
              sd_pov = sd(PovertyRate,
                          na.rm = TRUE),
              sd_inc = sd(medhhincome2014_tract,
                          na.rm = TRUE),
              sd_edu = sd(HSorlower,
                          na.rm = TRUE),
              sd_white = sd(pctwhitealone,
                            na.rm = TRUE))
  
  inner_join(A, B) %>% 
    mutate(Designated = case_when(Designated == 1 ~ "Opportunity Zone",
                                  T ~ "Other Tracts"))
}

ozcensus_combined_all <- combine_all(ozlist)

    # I was having trouble making the standard deviation in the same summarize
    # function as the means, so I just created a new tibble with the standard
    # deviations and joined them. I had to use case_when() to make sure that
    # each row has a state and that state is written accurately.

```

```{r log_model_ouput}
# Make a logistic model for each predictor; the posterior_predict output from
# that model can be mutated to give a likelihood that a census tract is
# designated an OZ, based on the predictor's value. This posterior_predict
# output will be used to make a chart on the model page of the shiny app.
  # Make the model and the posterior_predict output, with a generic name for
  # the variable that will go on the x axis.
    
    # Make the poverty rate model's output

      set.seed(9)
      pov_model <- ozlist %>% 
        stan_glm(Designated ~ PovertyRate,
                 refresh = 0,
                 family = binomial(link = "logit"),
                 data = .)
      
        # I used a binomial(link = "logit") family because the output of
        # Designated is binary (yes or no). At first, I included state as
        # another predictor. I saw that the trends were largely the same across
        # states. Given that, it would be confusing to show the trend for each
        # state on the same graph. (There were 6 very colorful lines overlapping
        # with each other.) However, if I include state as a predictor and do
        # not plot each state's trend separately, it is harder to see the trends
        # for the whole set of states. That's why I just removed state as a
        # predictor in the end.
    
      pov_new_obs <- 
        tibble(PovertyRate = rep(seq(0, 1, 0.01), 6),
               state = c(rep("Alabama", 101), rep("North Carolina", 101),
                         rep("South Carolina", 101), rep("Georgia", 101),
                         rep("Tennessee", 101), rep("Mississippi", 101)))
      
        # For the newdata, I made a tibble with each 0.01 interval between 0 and
        # 1, inclusive. This will help us measure the predictions for the full
        # range of possible poverty rates, splitting it up at pretty fine
        # intervals. I take a similar approach for each variable with is a
        # proportion of the population, but approach it differently for median
        # household income. Because there are 101 values between 0 and 1 with
        # 0.01 intervals, inclusive, I listed each state 101 times. This way,
        # the posterior_predict can consider this whole range for each state.
    
      set.seed(9)
      pov_mod_output <- 
        posterior_predict(pov_model, newdata = pov_new_obs) %>%
          as_tibble() %>%
          pivot_longer(cols =`1`:`606`,
                       names_to = "obs",
                       values_to = "pred_oz") %>% 
          group_by(obs) %>% 
          summarize(mean = mean(pred_oz),
                    .groups = "keep") %>% 
          mutate(as.numeric(obs)) %>% 
          arrange(`as.numeric(obs)`) %>% 
          bind_cols(pov_new_obs, .) %>%
          mutate(variable = PovertyRate) %>% 
          select(variable, state, mean)
        
        # Finally, I made a posterior_predict() output, turned it into a tibble,
        # and made the variable names generic so that it works well with the
        # ggplot commands in the model tab.
      
    # Make the low education rate model's output
      
        set.seed(9)  
        edu_model <- ozlist %>% 
            stan_glm(Designated ~ HSorlower,
                     refresh = 0,
                     family = binomial(link = "logit"),
                     data = .)
          
          edu_new_obs <- 
            tibble(HSorlower = rep(seq(0, 1, 0.01), 6),
                   state = c(rep("Alabama", 101),
                             rep("North Carolina", 101),
                             rep("South Carolina", 101),
                             rep("Georgia", 101),
                             rep("Tennessee", 101),
                             rep("Mississippi", 101)))
          
          set.seed(9)
          edu_mod_output <- 
            posterior_predict(edu_model, newdata = edu_new_obs) %>%
              as_tibble() %>%
              pivot_longer(cols =`1`:`606`,
                           names_to = "obs",
                           values_to = "pred_oz") %>% 
              group_by(obs) %>% 
              summarize(mean = mean(pred_oz),
                    .groups = "keep") %>% 
              mutate(as.numeric(obs)) %>% 
              arrange(`as.numeric(obs)`) %>% 
              bind_cols(edu_new_obs, .) %>%
              mutate(variable = HSorlower) %>% 
              select(variable, state, mean)

    # Make the median household income model's output
          
        set.seed(9)
        inc_model <- ozlist %>% 
          stan_glm(Designated ~ medhhincome2014_tract,
                   refresh = 0,
                   family = binomial(link = "logit"),
                   data = .)
        
        inc_new_obs <- 
          tibble(medhhincome2014_tract = rep(seq(0, 129000, 1000), 6),
                 state = c(rep("Alabama", 130),
                           rep("North Carolina", 130),
                           rep("South Carolina", 130),
                           rep("Georgia", 130),
                           rep("Tennessee", 130),
                           rep("Mississippi", 130)))
        
        # This time, I deviated from the pattern of using 0 to 1 by 0.01
        # intervals. I saw that the maximum median household income for a tract
        # in ozlist was a little over $129,000. So, I set this at my maximum, 0
        # as my minimum, and 1,000 as my interval. This allows for this
        # variable's output to include the whole range of potential values for
        # the relevant data set with good specificity. Also, because there are
        # now 130 values for median household income, I need to repeat each
        # state name 130 times.
        
        set.seed(9)
        inc_mod_output <- 
          posterior_predict(inc_model, newdata = inc_new_obs) %>%
            as_tibble() %>%
            pivot_longer(cols =`1`:`780`,
                         names_to = "obs",
                         values_to = "pred_oz") %>% 
            group_by(obs) %>% 
            summarize(mean = mean(pred_oz),
                    .groups = "keep") %>% 
            mutate(as.numeric(obs)) %>% 
            arrange(`as.numeric(obs)`) %>% 
            bind_cols(inc_new_obs, .) %>%
            mutate(variable = medhhincome2014_tract) %>% 
            select(variable, state, mean)
    
    # Make the white percent of population model's output
        
        set.seed(9)
        white_model <- ozlist %>% 
          stan_glm(Designated ~ pctwhitealone,
                   refresh = 0,
                   family = binomial(link = "logit"),
                   data = .)
        
        white_new_obs <- 
          tibble(pctwhitealone = rep(seq(0, 1, 0.01), 6),
                 state = c(rep("Alabama", 101),
                           rep("North Carolina", 101),
                           rep("South Carolina", 101),
                           rep("Georgia", 101),
                           rep("Tennessee", 101),
                           rep("Mississippi", 101)))

        set.seed(9)
        white_mod_output <- 
          posterior_predict(white_model, newdata = white_new_obs) %>%
            as_tibble() %>%
            pivot_longer(cols =`1`:`606`,
                         names_to = "obs",
                         values_to = "pred_oz") %>% 
            group_by(obs) %>% 
            summarize(mean = mean(pred_oz),
                    .groups = "keep") %>% 
            mutate(as.numeric(obs)) %>% 
            arrange(`as.numeric(obs)`) %>% 
            bind_cols(white_new_obs, .) %>%
            mutate(variable = pctwhitealone) %>% 
            select(variable, state, mean)

    # Make the unemployment rate model's output
        
        set.seed(9)
        ue_model <- ozlist %>% 
          stan_glm(Designated ~ unemprate,
                   refresh = 0,
                   family = binomial(link = "logit"),
                   data = .)
        
        ue_new_obs <-
          tibble(unemprate = rep(seq(0, 1, 0.01), 6),
                 state = c(rep("Alabama", 101),
                           rep("North Carolina", 101),
                           rep("South Carolina", 101),
                           rep("Georgia", 101),
                           rep("Tennessee", 101),
                           rep("Mississippi", 101)))

        set.seed(9)    
        ue_mod_output <- 
          posterior_predict(ue_model, newdata = ue_new_obs) %>%
            as_tibble() %>%
            pivot_longer(cols =`1`:`606`,
                         names_to = "obs",
                         values_to = "pred_oz") %>% 
            group_by(obs) %>% 
            summarize(mean = mean(pred_oz),
                    .groups = "keep") %>% 
            mutate(as.numeric(obs)) %>% 
            arrange(`as.numeric(obs)`) %>% 
            bind_cols(ue_new_obs, .) %>%
            mutate(variable = unemprate) %>% 
            select(variable, state, mean)
```

```{r create .Rds for all objects}
# Now that I have created all the models that I need, I can write an .Rds with
# all of them, then read that .Rds at the start of final_project.R. If I don't
# do this, then I cannot publish the app successfully. That is because I would
# have to run clean_data.Rmd in order to run final_project.R, which I can only
# do locally. This step allows me to successfully publish my work.

all_objects_for_rds <- list(pov_mod = pov_model, pov_output = pov_mod_output,
                    inc_mod = inc_model, inc_output = inc_mod_output,
                    white_mod = white_model, white_output = white_mod_output,
                    ue_mod =  ue_model, ue_output = ue_mod_output,
                    edu_mod = edu_model, edu_output = edu_mod_output,
                    ozcensus_combined_all = ozcensus_combined_all)

saveRDS(all_objects_for_rds, "Gov50final_data/all_objects.rds")
```

